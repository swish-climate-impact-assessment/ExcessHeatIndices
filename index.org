#+TITLE:Excess Heat Indices 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* COMMENT TODO-list
** TODO fix lags using zoo package, rollmean
** TODO add apparent temperature
* Introduction
#+name:README.md
#+begin_src markdown :tangle README.md :exports none :eval no
Excess Heat Indices	
-------------------

During 2011 I worked for Geoff Morgan (Geoff.Morgan@ncahs.health.nsw.gov.au) on a consultancy with NSW health to look at heatwaves, mortality and admissions. We use the percentiles of daily max temperature and apparent temperature in a similar way to the paper by Behnoosh Khalaj and Keith Dear. In additional sensitivity analyses we also developed material related to a newly proposed heatwave metric called the Excess Heat Factor by John Nairn at the BoM.

The reports/EHIs_transformations_doc.Rnw file is an Sweave document which contains the complete text and R codes that you can execute and produce the PDF (also found in the reports directory).  The interested reader is encouraged to run the R codes to do the calculations and generate the graphs that get compiled into that pdf file.  These R codes are also held separately in the src directory and can be evaluated in the correct sequence using the go.r script if you prefer.  Please don't hesitate to send me queries or comments on the algorithms or other aspects of this work.

Some Background
---------------

We were asked by our NSW health collaborators to investigate some heatwave indices developed by the BoM. NSW BoM like the look of three indices invented at the SA BoM office (by John Nairn) - they want to construct a national definition. Apparently BoM central HQ like John's definition the most (not published in a journal yet, the best ref is http://www.cawcr.gov.au/events/modelling_workshops/workshop_2009/papers/NAIRN.pdf). 

John has worked with PriceWaterhouseCoopers to apply the heatwave in a recent report http://www.pwc.com.au/industry/government/assets/extreme-heat-events-nov11.pdf

Ivan Hanigan
2012-04-21
#+end_src

* Codes
** ExcessHeatIndices-package.Rd
#+name:ExcessHeatIndices-package.Rd
#+begin_src markdown  :tangle man/ExcessHeatIndices-package.Rd :exports none :eval no
    \name{ExcessHeatIndices-package}
\alias{ExcessHeatIndices-package}
    \alias{ExcessHeatIndices}
\docType{package}
    \title{
Excess Heat Indices 
    ~~ package title ~~
}
    \description{
Excess Heat Indices for Human Health research
    ~~ A concise (1-5 lines) description of the package ~~
}
    \details{
\tabular{ll}{
    Package: \tab ExcessHeatIndices\cr
Type: \tab Package\cr
    Version: \tab 1.0\cr
Date: \tab 2013-01-30\cr
    License: \tab GPL2\cr
}
    ~~ An overview of how to use the package, including the most important functions ~~
}
    \author{
ivanhanigan
    
Maintainer: Who to complain to  ivan.hanigan@anu.edu.au 
    ~~ The author and/or maintainer of the package ~~
}
    \references{
~~ Literature or other references for background information ~~
    }

    \keyword{ package }
\seealso{
    ~~ Optional links to other man pages, e.g. ~~
~~ \code{\link[<pkg>:<pkg>-package]{<pkg>}} ~~
    }
\examples{
    ~~ simple examples of the most important functions ~~
}
    
#+end_src

** tests
#+name:tests
#+begin_src R :session *R* :tangle tests.r :exports none :eval no
  require(testthat)
  
  test_dir('tests', reporter = 'Summary')
  
#+end_src

** EHF
*** test
#+name:test-EHF
#+begin_src R :session *R* :tangle tests/test-EHF.r :exports none :eval no
  # first test
  dir()
  source('../R/EHF.r')
  require(swishdbtools)
  require(plyr)
  # access to ewedb is password restricted
  ch <- connect2postgres2('ewedb')
  slacode <- sql_subset(ch,"abs_sla.aussla01", subset = "sla_name = 'Scullin'",
             select = c("sla_code, sla_name"), eval=T)
  sql <- sql_subset(ch,"weather_sla.weather_sla",
                   subset=paste("sla_code = '",slacode$sla_code,"'",sep=""), eval = F)
  cat(sql)
  # this might take some minutes
  df <- dbGetQuery(ch, sql)
  head(df)
  tail(df)
  with(df, plot(date, maxave))
  str(df)
  df2 <- EHF(df, 'maxave', "date", min(df$date), max(df$date))
  names(df2)
  hist(subset(df2, EHF >= 1)[,'EHF'])
  threshold <- quantile(subset(df2, EHF >= 1)[,'EHF'], probs=0.9)
  
  with(df, plot(date, maxave, type = 'l'))
  with(subset(df2, EHF > threshold), points(date, maxave, col = 'red', pch = 16))
  
#+end_src

*** R
#+name:EHF
#+begin_src R :session *R* :tangle R/EHF.r :exports none :eval no
###############################################################################
 if (!require(Hmisc)) install.packages('Hmisc', repos='http://cran.csiro.au'); require(Hmisc)
 EHF <- function(analyte = data_subset,
  exposurename = 'air_temperature_in_degrees_c_max_climatezone_av',
  datename = 'date',
  referencePeriodStart = as.Date('1971-1-1'),
  referencePeriodEnd = as.Date('2000-12-31'),
  nlags = 32) {
  # TASK SHOULD WE IMPUTE MISSING DAYS?
 
  # first get lags
  # TASK THERE IS PROBABLY A VECTORISED VERSION THAT IS QUICKER?
  # TASK it is rollmean from the zoo package
  # ALTHOUGH THAT DOESNT HANDLE NAs SO TRY ROLLAPPLY?
  analyte$temp_lag0 <- analyte[,exposurename]
  exposuresList <- 'temp_lag0'
  # make sure in order
  analyte <- arrange(analyte,  analyte[,datename])
  # lag0 is not needed
  for(lagi in 1:nlags){
 	# lagi <- 1
 	exposuresList <- c(exposuresList, gsub('lag0',paste('lag', lagi,sep=''), exposuresList[1]))
 	analyte[,(ncol(analyte)+1)] <- Lag(analyte[,exposuresList[1]],lagi)
 	}
  exposuresList <- exposuresList[-1]
  names(analyte) <- c(names(analyte[,1:(ncol(analyte)-nlags)]),exposuresList)
  # head(analyte)
  # now 3 day av
  analyte$temp_movav <- rowMeans(analyte[,c('temp_lag0','temp_lag1','temp_lag2')], na.rm =FALSE)

  # now 30 day av
  # paste('temp_lag',3:32, sep = '', collapse = \"','\")
  analyte$temp30_movav <- rowMeans(analyte[,c('temp_lag3','temp_lag4','temp_lag5','temp_lag6','temp_lag7','temp_lag8','temp_lag9','temp_lag10','temp_lag11','temp_lag12','temp_lag13','temp_lag14','temp_lag15','temp_lag16','temp_lag17','temp_lag18','temp_lag19','temp_lag20','temp_lag21','temp_lag22','temp_lag23','temp_lag24','temp_lag25','temp_lag26','temp_lag27','temp_lag28','temp_lag29','temp_lag30','temp_lag31','temp_lag32')], na.rm =FALSE)
  # TASK note that this removes any missing days which could be imputed
  analyte <- na.omit(analyte)
  # head(analyte)
 
  # now calculate the EHI
  analyte$EHIaccl <- analyte$temp_movav - analyte$temp30_movav
  
  # first calculate the 95th centile
  referencestart <- referencePeriodStart
  referenceend <- referencePeriodEnd
  analyte$dateidCol <- analyte[,datename]
  reference <- subset(analyte, dateidCol >= referencestart & dateidCol <= referenceend, select = c('dateidCol', exposurename))
  head(reference);tail(reference)
  T95 <- quantile(reference[,exposurename], 0.95, na.rm = T)
  T95
 
  # now calculate the EHIsig
  analyte$EHIsig <- analyte$temp_movav - T95
  
  # now calculate the EHF
  analyte$EHF <- abs(analyte$EHIaccl) * analyte$EHIsig
  
  # proposed integrations
  # counts can be done quicker with this
  x <- analyte$EHIaccl >= 0
  xx <- (cumsum(!x) + 1) * x 
  x2<-(seq_along(x) - match(xx, xx) + 1) * x 
  analyte$EHIacclCount <- x2

  # alternately, slower but more interpretable
  # analyte$EHIacclCount2<-as.numeric(0)
  # # 
  # which(analyte$dates == as.Date('2009-1-1'))
  # which(analyte$dates == as.Date('2009-3-1'))
  
  # for(j in 43034:43093){
  # # j=43034
  # analyte$EHIacclCount2[j] <- ifelse(analyte$EHIaccl[j] < 0, 0,
  # ifelse(analyte$EHIaccl[j-1] >= 0, 1 + analyte$EHIacclCount2[j-1],
  # 1)
  # )
  # }
  
  x <- analyte$EHIsig >= 0
  xx <- (cumsum(!x) + 1) * x 
  x2<-(seq_along(x) - match(xx, xx) + 1) * x 
  analyte$EHIsigCount <- x2
  
  # sums
  EHFinverted  <- analyte$EHF * -1 
  y <- ifelse(EHFinverted >= 0, 0, analyte$EHF)
  f <- EHFinverted < 0
  f <- (cumsum(!f) + 1) * f 
  z <- unsplit(lapply(split(y,f),cumsum),f)
  analyte$EHFintegrated <- z
  
  # alternately, slower but more interpretable
  # analyte$EHFintegrated2 <- as.numeric(0)
  # for(j in 43034:43093){
  # # j = 43034
	# analyte$EHFintegrated2[j] <- ifelse(analyte$EHF[j] < 0,0,
	 # ifelse(analyte$EHF[j-1] >= 0,
	 # analyte$EHF[j] + analyte$EHFintegrated2[j-1],
	 # analyte$EHF[j])
	 # )
	# }
  
  return(analyte)
  }
 

#+end_src

*** man
#+name:EHF
#+begin_src markdown :tangle man/EHF.Rd :exports none :eval no
\name{EHF}
\alias{EHF}
%- Also NEED an '\alias' for EACH other topic documented here.
\title{
Excess Heat Factor
}
\description{
The EHF is an extension to a high pass filter, compared with long term percentiles.
}
\usage{
EHF(analyte = data_subset, exposurename = "air_temperature_in_degrees_c_max_climatezone_av", datename = "date", referencePeriodStart = as.Date("1971-1-1"), referencePeriodEnd = as.Date("2000-12-31"), nlags = 32)
}
%- maybe also 'usage' for other objects documented here.
\arguments{
  \item{analyte}{
dataframe
}
  \item{exposurename}{
the name of the exposure variable in the dataframe
}
  \item{datename}{
usually just date
}
  \item{referencePeriodStart}{
start of baseline climate reference period
}
  \item{referencePeriodEnd}{
end of baseline
}
  \item{nlags}{
number of lags, default is 32
}
}
\details{

}
\value{
A dataframe.
}
\references{
%% ~put references to the literature/web site here ~
}
\author{
ivanhanigan, original by John Nairn (Australian Bureau of Meteorology)
}
\note{
%%  ~~further notes~~
}



\seealso{
%% ~~objects to See Also as \code{\link{help}}, ~~~
}
\examples{

output <- EHF(analyte = data_subset, exposurename = "air_temperature_in_degrees_c_max_climatezone_av", 
    datename = "date", referencePeriodStart = as.Date("1971-1-1"), 
    referencePeriodEnd = as.Date("2000-12-31"), nlags = 32) 

}
% Add one or more standard keywords, see file 'KEYWORDS' in the
% R documentation directory.
\keyword{ ~kwd1 }
\keyword{ ~kwd2 }% __ONLY ONE__ keyword per line

#+end_src
* CaseStudies
** COMMENT 2013-12-06-auto-download-bureau-meteorology-diurnal-data
#+name:auto-download-bureau-meteorology-diurnal-data-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-06-auto-download-bureau-meteorology-diurnal-data.md :exports none :eval no :padline no
  ---
  name: 2013-12-06-auto-download-bureau-meteorology-diurnal-data
  layout: post
  title: auto-download-bureau-meteorology-diurnal-data
  date: 2013-12-06
  categories:
  - extreme weather events
  - excess heat indices
  ---
  <head>
  <title>Excess Heat Indices </title>
  <meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
  <meta name="title" content="Excess Heat Indices "/>
  <meta name="generator" content="Org-mode"/>
  <meta name="generated" content="2013-12-07T23:25+1100"/>
  <meta name="author" content="Ivan Hanigan"/>
  <meta name="description" content=""/>
  <meta name="keywords" content=""/>
  <style type="text/css">
   <!--/*--><![CDATA[/*><!--*/
    html { font-family: Times, serif; font-size: 12pt; }
    .title  { text-align: center; }
    .todo   { color: red; }
    .done   { color: green; }
    .tag    { background-color: #add8e6; font-weight:normal }
    .target { }
    .timestamp { color: #bebebe; }
    .timestamp-kwd { color: #5f9ea0; }
    .right  {margin-left:auto; margin-right:0px;  text-align:right;}
    .left   {margin-left:0px;  margin-right:auto; text-align:left;}
    .center {margin-left:auto; margin-right:auto; text-align:center;}
    p.verse { margin-left: 3% }
    pre {
          border: 1pt solid #AEBDCC;
          background-color: #F3F5F7;
          padding: 5pt;
          font-family: courier, monospace;
          font-size: 90%;
          overflow:auto;
    }
    table { border-collapse: collapse; }
    td, th { vertical-align: top;  }
    th.right  { text-align:center;  }
    th.left   { text-align:center;   }
    th.center { text-align:center; }
    td.right  { text-align:right;  }
    td.left   { text-align:left;   }
    td.center { text-align:center; }
    dt { font-weight: bold; }
    div.figure { padding: 0.5em; }
    div.figure p { text-align: center; }
    div.inlinetask {
      padding:10px;
      border:2px solid gray;
      margin:10px;
      background: #ffffcc;
    }
    textarea { overflow-x: auto; }
    .linenr { font-size:smaller }
    .code-highlighted {background-color:#ffff00;}
    .org-info-js_info-navigation { border-style:none; }
    #org-info-js_console-label { font-size:10px; font-weight:bold;
                                 white-space:nowrap; }
    .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                   font-weight:bold; }
    /*]]>*/-->
  </style>
  <script type="text/javascript">
  /*
  @licstart  The following is the entire license notice for the
  JavaScript code in this tag.
  
  Copyright (C) 2012-2013 Free Software Foundation, Inc.
  
  The JavaScript code in this tag is free software: you can
  redistribute it and/or modify it under the terms of the GNU
  General Public License (GNU GPL) as published by the Free Software
  Foundation, either version 3 of the License, or (at your option)
  any later version.  The code is distributed WITHOUT ANY WARRANTY;
  without even the implied warranty of MERCHANTABILITY or FITNESS
  FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
  
  As additional permission under GNU GPL version 3 section 7, you
  may distribute non-source (e.g., minimized or compacted) forms of
  that code without the copy of the GNU GPL normally required by
  section 4, provided you include this license notice and a URL
  through which recipients can access the Corresponding Source.
  
  
  @licend  The above is the entire license notice
  for the JavaScript code in this tag.
  ,*/
  <!--/*--><![CDATA[/*><!--*/
   function CodeHighlightOn(elem, id)
   {
     var target = document.getElementById(id);
     if(null != target) {
       elem.cacheClassElem = elem.className;
       elem.cacheClassTarget = target.className;
       target.className = "code-highlighted";
       elem.className   = "code-highlighted";
     }
   }
   function CodeHighlightOff(elem, id)
   {
     var target = document.getElementById(id);
     if(elem.cacheClassElem)
       elem.className = elem.cacheClassElem;
     if(elem.cacheClassTarget)
       target.className = elem.cacheClassTarget;
   }
  /*]]>*///-->
  </script>
  
  </head>
  <body>
  
  <div id="preamble">
  
  </div>
  
  <div id="content">
  <h1 class="title">Excess Heat Indices </h1>
  
  
  <div id="table-of-contents">
  <h2>Table of Contents</h2>
  <div id="text-table-of-contents">
  <ul>
  <li><a href="#sec-1">1 auto-download-bureau-meteorology-diurnal-data</a>
  <ul>
  <li><a href="#sec-1-1">1.1 First the FTP server URL structure</a></li>
  <li><a href="#sec-1-2">1.2 table</a></li>
  <li><a href="#sec-1-3">1.3 R Code: bom<sub>download</sub>.r</a></li>
  <li><a href="#sec-1-4">1.4 R Code: bom<sub>collation</sub>.r</a></li>
  <li><a href="#sec-1-5">1.5 BAT file (windoze)</a></li>
  <li><a href="#sec-1-6">1.6 check the data</a></li>
  <li><a href="#sec-1-7">1.7 Conclusions</a></li>
  </ul>
  </li>
  </ul>
  </div>
  </div>
  
  <div id="outline-container-1" class="outline-3">
  <h3 id="sec-1"><span class="section-number-3">1</span> auto-download-bureau-meteorology-diurnal-data</h3>
  <div class="outline-text-3" id="text-1">
  
  
  <ul>
  <li>We;re looking at health impacts of high temperatures at work 
  </li>
  <li>need to see the highest temperatures during the working hours
  </li>
  <li>bom provides hourly data for download, but only 3 days at a time
  </li>
  <li>we build a script and set it on a schedule to run every day, download the data and collate the results
  </li>
  </ul>
  
  
  
  </div>
  
  <div id="outline-container-1-1" class="outline-4">
  <h4 id="sec-1-1"><span class="section-number-4">1.1</span> First the FTP server URL structure</h4>
  <div class="outline-text-4" id="text-1-1">
  
  
  <ul>
  <li>The URLS are predictable, just need the station id, state and a code if metro or rural
  </li>
  </ul>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-1-2" class="outline-4">
  <h4 id="sec-1-2"><span class="section-number-4">1.2</span> table</h4>
  <div class="outline-text-4" id="text-1-2">
  
  <table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
  <colgroup><col class="right" /><col class="left" /><col class="right" />
  </colgroup>
  <tbody>
  <tr><td class="right">Station<sub>ID</sub></td><td class="left">State</td><td class="right">City<sub>9</sub><sub>or</sub><sub>regional</sub><sub>8</sub>_</td></tr>
  <tr><td class="right">94774</td><td class="left">N</td><td class="right">9</td></tr>
  <tr><td class="right">95719</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">94768</td><td class="left">N</td><td class="right">9</td></tr>
  <tr><td class="right">94763</td><td class="left">N</td><td class="right">9</td></tr>
  <tr><td class="right">94767</td><td class="left">N</td><td class="right">9</td></tr>
  <tr><td class="right">94910</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">94929</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">95896</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">94693</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">94691</td><td class="left">N</td><td class="right">8</td></tr>
  <tr><td class="right">95677</td><td class="left">S</td><td class="right">9</td></tr>
  <tr><td class="right">94675</td><td class="left">S</td><td class="right">9</td></tr>
  <tr><td class="right">94672</td><td class="left">S</td><td class="right">9</td></tr>
  <tr><td class="right">94866</td><td class="left">V</td><td class="right">9</td></tr>
  <tr><td class="right">95867</td><td class="left">V</td><td class="right">9</td></tr>
  <tr><td class="right">94868</td><td class="left">V</td><td class="right">9</td></tr>
  <tr><td class="right">94875</td><td class="left">V</td><td class="right">8</td></tr>
  </tbody>
  </table>
  
  
  
  
  <ul>
  <li>now create a script called "bom<sub>download</sub>.r"
  </li>
  <li>it takes the station details and paste into the URLs
  </li>
  <li>downloads the files
  </li>
  <li>stores in a directory for each days downloads
  </li>
  </ul>
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-1-3" class="outline-4">
  <h4 id="sec-1-3"><span class="section-number-4">1.3</span> R Code: bom<sub>download</sub>.r</h4>
  <div class="outline-text-4" id="text-1-3">
  
  
  
  
  <pre class="src src-R">filename = <span style="color: #2aa198;">"~/data/ExcessHeatIndices/inst/doc/weather_stations.csv"</span>
  output_directory = <span style="color: #2aa198;">"~/bom-downloads"</span>
  setwd(output_directory)
  
  urls <span style="color: #268bd2; font-weight: bold;">&lt;-</span> read.csv(filename)
  urls_list <span style="color: #268bd2; font-weight: bold;">&lt;-</span> paste(sep = <span style="color: #2aa198;">""</span>, <span style="color: #2aa198;">"http://www.bom.gov.au/fwo/ID"</span>,
                    urls$State,
                    <span style="color: #2aa198;">"60"</span>, 
                    urls$City_9_or_regional_8_,
                    <span style="color: #2aa198;">"01/ID"</span>,
                    urls$State,
                    <span style="color: #2aa198;">"60"</span>,
                    urls$City_9_or_regional_8_,
                    <span style="color: #2aa198;">"01."</span>,
                    urls$Station_ID,
                    <span style="color: #2aa198;">".axf"</span>)
  
  output_directory <span style="color: #268bd2; font-weight: bold;">&lt;-</span> file.path(output_directory,Sys.Date())
  dir.create(output_directory)
  
  <span style="color: #859900; font-weight: bold;">for</span>(url <span style="color: #859900; font-weight: bold;">in</span> urls_list)
  {
    output_file <span style="color: #268bd2; font-weight: bold;">&lt;-</span> file.path(output_directory,basename(url))
    download.file(url, output_file, mode = <span style="color: #2aa198;">"wb"</span>)
  
  }
  print(<span style="color: #2aa198;">"SUCCESS"</span>)
  
  </pre>
  
  
  
  
  <ul>
  <li>Now the data can be combined
  </li>
  <li>clean up the header and extraneous extra line at the bottom
  </li>
  </ul>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-1-4" class="outline-4">
  <h4 id="sec-1-4"><span class="section-number-4">1.4</span> R Code: bom<sub>collation</sub>.r</h4>
  <div class="outline-text-4" id="text-1-4">
  
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;"># </span><span style="color: #586e75;">this takes data in directories from bom_download.r</span>
   
  <span style="color: #586e75;"># </span><span style="color: #586e75;">first get list of directories</span>
  filelist <span style="color: #268bd2; font-weight: bold;">&lt;-</span> dir(pattern = <span style="color: #2aa198;">"axf"</span>, recursive = T)
  filelist
   
  <span style="color: #586e75;"># </span><span style="color: #586e75;">next get directories for days we haven't done yet</span>
  <span style="color: #859900; font-weight: bold;">if</span>(file.exists(<span style="color: #2aa198;">"complete_dataset.csv"</span>))
  {
  complete_data <span style="color: #268bd2; font-weight: bold;">&lt;-</span> read.csv(<span style="color: #2aa198;">"complete_dataset.csv"</span>, stringsAsFactors = F)
  <span style="color: #586e75;">#</span><span style="color: #586e75;">str(complete_data)</span>
  last_collated <span style="color: #268bd2; font-weight: bold;">&lt;-</span> max(as.Date(complete_data$date_downloaded))
  <span style="color: #586e75;">#</span><span style="color: #586e75;">max(complete_data$local_hrmin)</span>
   
  days_downloaded <span style="color: #268bd2; font-weight: bold;">&lt;-</span> dirname(filelist)
  filelist <span style="color: #268bd2; font-weight: bold;">&lt;-</span> filelist[which(as.Date(days_downloaded) &gt; as.Date(last_collated))]
  }
   
  <span style="color: #586e75;"># </span><span style="color: #586e75;">for these collate them into the complete file</span>
  <span style="color: #859900; font-weight: bold;">for</span>(f <span style="color: #859900; font-weight: bold;">in</span> filelist)
  {
    <span style="color: #586e75;">#</span><span style="color: #586e75;">f &lt;- filelist[2]</span>
    print(f)
    fin <span style="color: #268bd2; font-weight: bold;">&lt;-</span> read.csv(f, colClasses = c(<span style="color: #2aa198;">"local_date_time_full.80."</span> = <span style="color: #2aa198;">"character"</span>), 
      stringsAsFactors = F, skip = 19)
    fin <span style="color: #268bd2; font-weight: bold;">&lt;-</span> fin[1:(nrow(fin) - 1),]
    fin$date_downloaded <span style="color: #268bd2; font-weight: bold;">&lt;-</span> dirname(f)
    fin$local_year <span style="color: #268bd2; font-weight: bold;">&lt;-</span> substr(fin$local_date_time_full.80., 1, 4)
    fin$local_month <span style="color: #268bd2; font-weight: bold;">&lt;-</span> substr(fin$local_date_time_full.80., 5, 6)
    fin$local_day <span style="color: #268bd2; font-weight: bold;">&lt;-</span> substr(fin$local_date_time_full.80., 7, 8)
    fin$local_hrmin <span style="color: #268bd2; font-weight: bold;">&lt;-</span> substr(fin$local_date_time_full.80., 9, 12)
    fin$local_date <span style="color: #268bd2; font-weight: bold;">&lt;-</span> paste(fin$local_year, fin$local_month, fin$local_day, sep = <span style="color: #2aa198;">"-"</span>)
    <span style="color: #859900; font-weight: bold;">if</span>(file.exists(<span style="color: #2aa198;">"complete_dataset.csv"</span>))
    {
    write.table(fin, <span style="color: #2aa198;">"complete_dataset.csv"</span>, row.names = F, sep = <span style="color: #2aa198;">","</span>, append = T, col.names = F)
    } <span style="color: #859900; font-weight: bold;">else</span> {
    write.table(fin, <span style="color: #2aa198;">"complete_dataset.csv"</span>, row.names = F, sep = <span style="color: #2aa198;">","</span>)
    }
  }
  </pre>
  
  
  <ul>
  <li>so now let;s automate the process
  </li>
  <li>make a BAT file
  </li>
  </ul>
  
  
  </div>
  
  </div>
  
  <div id="outline-container-1-5" class="outline-4">
  <h4 id="sec-1-5"><span class="section-number-4">1.5</span> BAT file (windoze)</h4>
  <div class="outline-text-4" id="text-1-5">
  
  
  
  
  
  <pre class="src src-R"><span style="color: #2aa198;">"C:\Program Files\R\R-2.15.2\bin\Rscript.exe"</span> <span style="color: #2aa198;">"~\bom-downloads\bom_download.r"</span>
  </pre>
  
  
  <ul>
  <li>add this  bat file to the scheduled tasks in your control panel
  </li>
  <li>use chron for a linux version
  </li>
  </ul>
  
  
  
  </div>
  
  </div>
  
  <div id="outline-container-1-6" class="outline-4">
  <h4 id="sec-1-6"><span class="section-number-4">1.6</span> check the data</h4>
  <div class="outline-text-4" id="text-1-6">
  
  
  
  
  <pre class="src src-R"><span style="color: #586e75;">#### </span><span style="color: #586e75;">name:check the data ####</span>
  <span style="color: #268bd2; font-weight: bold;">require</span>(plyr)
  
  setwd(<span style="color: #2aa198;">"~/bom-downloads"</span>)
  <span style="color: #268bd2; font-weight: bold;">source</span>(<span style="color: #2aa198;">"bom_download.r"</span>)
  dir()
  <span style="color: #268bd2; font-weight: bold;">source</span>(<span style="color: #2aa198;">"bom_collation.r"</span>)
  
  complete_data <span style="color: #268bd2; font-weight: bold;">&lt;-</span> read.csv(<span style="color: #2aa198;">"complete_dataset.csv"</span>, stringsAsFactors = F)
  str(complete_data)
  
  <span style="color: #586e75;"># </span><span style="color: #586e75;">Quick and dirty de-duplication</span>
  table(complete_data$name.80.)
  qc <span style="color: #268bd2; font-weight: bold;">&lt;-</span> subset(complete_data, name.80. == <span style="color: #2aa198;">"Broken Hill Airport"</span>)
  qc <span style="color: #268bd2; font-weight: bold;">&lt;-</span> ddply(qc, <span style="color: #2aa198;">"local_date_time_full.80."</span>,
    summarise, apparent_temp = mean(apparent_t))
  
  names(qc)
  png(<span style="color: #2aa198;">"qc-diurnal-plot.png"</span>)
  with(qc,
       plot(apparent_temp, type= <span style="color: #2aa198;">"l"</span>)
       )
  dev.off()
  </pre>
  
  
  <p>
  <img src="/images/qc-diurnal-plot.png"  alt="qc-diurnal-plot.png" />
  </p>
  </div>
  
  </div>
  
  <div id="outline-container-1-7" class="outline-4">
  <h4 id="sec-1-7"><span class="section-number-4">1.7</span> Conclusions</h4>
  <div class="outline-text-4" id="text-1-7">
  
  
  <ul>
  <li>watch the data roll on in
  </li>
  <li>each day there are about 3 days downloaded
  </li>
  <li>meaning duplicates will be frequent, need to write a script to de-duplicate
  </li>
  <li>cheers!
  </li>
  </ul>
  
  
  </div>
  </div>
  </div>
  </div>
  
  </body>
  </html>
  
#+end_src

** COMMENT DEPRECATED
#+name:auto-download-bureau-meteorology-diurnal-data-header
#+begin_src R :session *R* :tangle no :exports none :eval no :padline no
  
  - We;re looking at health impacts of high temperatures at work 
  - need to see the highest temperatures during the working hours
  - bom provides hourly data for download, but only 3 days at a time
  - we build a script and set it on a schedule to run every day, download the data and collate the results
  
  #### First the FTP server URL structure
  
  - The URLS are predictable, just need the station id, state and a code if metro or rural
  
  #### table
      | Station_ID | State | City_9_or_regional_8_ |
      |      94774 | N     |                     9 |
      |      95719 | N     |                     8 |
      |      94768 | N     |                     9 |
      |      94763 | N     |                     9 |
      |      94767 | N     |                     9 |
      |      94910 | N     |                     8 |
      |      94929 | N     |                     8 |
      |      95896 | N     |                     8 |
      |      94693 | N     |                     8 |
      |      94691 | N     |                     8 |
      |      95677 | S     |                     9 |
      |      94675 | S     |                     9 |
      |      94672 | S     |                     9 |
      |      94866 | V     |                     9 |
      |      95867 | V     |                     9 |
      |      94868 | V     |                     9 |
      |      94875 | V     |                     8 |
  
  
  <p></p> 
  
  - now create a script called "bom_download.r"
  - it takes the station details and paste into the URLs
  - downloads the files
  - stores in a directory for each days downloads
  
  #### R Code:
      filename = "~/data/ExcessHeatIndices/inst/doc/weather_stations.csv"
      output_directory = "~/bom-downloads"
      setwd(output_directory)
      
      urls <- read.csv(filename)
      urls_list <- paste(sep = "", "http://www.bom.gov.au/fwo/ID",
                        urls$State,
                        "60", 
                        urls$City_9_or_regional_8_,
                        "01/ID",
                        urls$State,
                        "60",
                        urls$City_9_or_regional_8_,
                        "01.",
                        urls$Station_ID,
                        ".axf")
      
      output_directory <- file.path(output_directory,Sys.Date())
      dir.create(output_directory)
  
      for(url in urls_list)
      {
        output_file <- file.path(output_directory,basename(url))
        download.file(url, output_file, mode = "wb")
      
      }
      print("SUCCESS")
  
  <p></p> 
  
  - Now the data can be combined
  - clean up the header and extraneous extra line at the bottom
  
  #### R Code:
      # this takes data in directories from bom_download.r
       
      # first get list of directories
      filelist <- dir(pattern = "axf", recursive = T)
      filelist
       
      # next get directories for days we haven't done yet
      if(file.exists("complete_dataset.csv"))
      {
      complete_data <- read.csv("complete_dataset.csv", stringsAsFactors = F)
      #str(complete_data)
      last_collated <- max(as.Date(complete_data$date_downloaded))
      #max(complete_data$local_hrmin)
       
      days_downloaded <- dirname(filelist)
      filelist <- filelist[which(as.Date(days_downloaded) > as.Date(last_collated))]
      }
       
      # for these collate them into the complete file
      for(f in filelist)
      {
        f <- filelist[2]
        print(f)
        fin <- read.csv(f, colClasses = c("local_date_time_full.80." = "character"), stringsAsFactors = F, skip = 19)
        fin <- fin[1:(nrow(fin) - 1),]
        fin$date_downloaded <- dirname(f)
        fin$local_year <- substr(fin$local_date_time_full.80., 1, 4)
        fin$local_month <- substr(fin$local_date_time_full.80., 5, 6)
        fin$local_day <- substr(fin$local_date_time_full.80., 7, 8)
        fin$local_hrmin <- substr(fin$local_date_time_full.80., 9, 12)
        fin$local_date <- paste(fin$local_year, fin$local_month, fin$local_day, sep = "-")
        if(file.exists("complete_dataset.csv"))
        {
        write.table(fin, "complete_dataset.csv", row.names = F, sep = ",", append = T, col.names = F)
        } else {
        write.table(fin, "complete_dataset.csv", row.names = F, sep = ",")
        }
      }
  
  <p></p>
  
  - so now let;s automate the process
  - make a BAT file
  
  #### BAT file (windoze)
      "C:\Program Files\R\R-2.15.2\bin\Rscript.exe" "~\bom-downloads\bom_download.r"
  
  <p></p>
  
  - add this  bat file to the scheduled tasks in your control panel
  
  #### Conclusions
  
  - watch the data roll on in
  - each day there are about 3 days downloaded
  - meaning duplicates will be frequent, need to write a script to de-duplicate
  - cheers!
#+end_src
** COMMENT table-stations-code
#+name:table-stations
#+begin_src R :session *R* :tangle no :exports reports :eval yes
  #### name:table-stations ####
  read.csv("inst/doc/weather_stations.csv")
#+end_src

** bom-downloads: auto-download-bureau-meteorology-diurnal-data

- We;re looking at health impacts of high temperatures at work 
- need to see the highest temperatures during the working hours
- bom provides hourly data for download, but only 3 days at a time
- we build a script and set it on a schedule to run every day, download the data and collate the results

*** First the FTP server URL structure

- The URLS are predictable, just need the station id, state and a code if metro or rural

*** table
    | Station_ID | State | City_9_or_regional_8_ |
    |      94774 | N     |                     9 |
    |      95719 | N     |                     8 |
    |      94768 | N     |                     9 |
    |      94763 | N     |                     9 |
    |      94767 | N     |                     9 |
    |      94910 | N     |                     8 |
    |      94929 | N     |                     8 |
    |      95896 | N     |                     8 |
    |      94693 | N     |                     8 |
    |      94691 | N     |                     8 |
    |      95677 | S     |                     9 |
    |      94675 | S     |                     9 |
    |      94672 | S     |                     9 |
    |      94866 | V     |                     9 |
    |      95867 | V     |                     9 |
    |      94868 | V     |                     9 |
    |      94875 | V     |                     8 |



- now create a script called "bom_download.r"
- it takes the station details and paste into the URLs
- downloads the files
- stores in a directory for each days downloads


*** R Code: bom_download.r
#+begin_src R :session *R* :tangle ~/data/ExcessHeatIndices/inst/doc/bom_download.r :exports reports :eval no

    filename = "~/data/ExcessHeatIndices/inst/doc/weather_stations.csv"
    output_directory = "~/bom-downloads"
    setwd(output_directory)
    
    urls <- read.csv(filename)
    urls_list <- paste(sep = "", "http://www.bom.gov.au/fwo/ID",
                      urls$State,
                      "60", 
                      urls$City_9_or_regional_8_,
                      "01/ID",
                      urls$State,
                      "60",
                      urls$City_9_or_regional_8_,
                      "01.",
                      urls$Station_ID,
                      ".axf")
    
    output_directory <- file.path(output_directory,Sys.Date())
    dir.create(output_directory)

    for(url in urls_list)
    {
      output_file <- file.path(output_directory,basename(url))
      download.file(url, output_file, mode = "wb")
    
    }
    print("SUCCESS")

#+end_src



- Now the data can be combined
- clean up the header and extraneous extra line at the bottom
  
*** R Code: bom_collation.r

#+begin_src R :session *R* :tangle ~/data/ExcessHeatIndices/inst/doc/bom_collation.r :exports reports :eval no
  
  # this takes data in directories from bom_download.r
   
  # first get list of directories
  filelist <- dir(pattern = "axf", recursive = T)
  filelist
   
  # next get directories for days we haven't done yet
  if(file.exists("complete_dataset.csv"))
  {
  complete_data <- read.csv("complete_dataset.csv", stringsAsFactors = F)
  #str(complete_data)
  last_collated <- max(as.Date(complete_data$date_downloaded))
  #max(complete_data$local_hrmin)
   
  days_downloaded <- dirname(filelist)
  filelist <- filelist[which(as.Date(days_downloaded) > as.Date(last_collated))]
  }
   
  # for these collate them into the complete file
  for(f in filelist)
  {
    #f <- filelist[2]
    print(f)
    fin <- read.csv(f, colClasses = c("local_date_time_full.80." = "character"), 
      stringsAsFactors = F, skip = 19)
    fin <- fin[1:(nrow(fin) - 1),]
    fin$date_downloaded <- dirname(f)
    fin$local_year <- substr(fin$local_date_time_full.80., 1, 4)
    fin$local_month <- substr(fin$local_date_time_full.80., 5, 6)
    fin$local_day <- substr(fin$local_date_time_full.80., 7, 8)
    fin$local_hrmin <- substr(fin$local_date_time_full.80., 9, 12)
    fin$local_date <- paste(fin$local_year, fin$local_month, fin$local_day, sep = "-")
    if(file.exists("complete_dataset.csv"))
    {
    write.table(fin, "complete_dataset.csv", row.names = F, sep = ",", append = T, col.names = F)
    } else {
    write.table(fin, "complete_dataset.csv", row.names = F, sep = ",")
    }
  }
#+end_src
  
- so now let;s automate the process
- make a BAT file
  
*** BAT file (windoze)

#+name:BAT
#+begin_src R :session *R* :tangle ~/data/ExcessHeatIndices/inst/doc/bom_download.bat :exports reports :eval no
"C:\Program Files\R\R-2.15.2\bin\Rscript.exe" "~\bom-downloads\bom_download.r"
#+end_src
  
- add this  bat file to the scheduled tasks in your control panel
- use chron for a linux version


*** check the data
#+name:check the data
#+begin_src R :session *R* :tangle no :exports reports :eval no
  #### name:check the data ####
  require(plyr)
  
  setwd("~/bom-downloads")
  source("bom_download.r")
  dir()
  source("bom_collation.r")
  
  complete_data <- read.csv("complete_dataset.csv", stringsAsFactors = F)
  str(complete_data)
  
  # Quick and dirty de-duplication
  table(complete_data$name.80.)
  qc <- subset(complete_data, name.80. == "Broken Hill Airport")
  qc <- ddply(qc, "local_date_time_full.80.",
    summarise, apparent_temp = mean(apparent_t))
  
  names(qc)
  png("qc-diurnal-plot.png")
  with(qc,
       plot(apparent_temp, type= "l")
       )
  dev.off()
#+end_src

[[file:qc-diurnal-plot.png]]

*** Conclusions
  
- watch the data roll on in
- each day there are about 3 days downloaded
- meaning duplicates will be frequent, need to write a script to de-duplicate
- cheers!

