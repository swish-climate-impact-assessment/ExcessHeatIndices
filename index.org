#+TITLE:Excess Heat Indices 
#+AUTHOR: Ivan Hanigan
#+email: ivan.hanigan@anu.edu.au
#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [a4paper]
#+LATEX: \tableofcontents
-----
* TODO-list
** TODO fix lags using zoo package, rollmean
** TODO add apparent temperature
* Introduction
#+name:README.md
#+begin_src markdown :tangle README.md :exports none :eval no
Excess Heat Indices	
-------------------

During 2011 I worked for Geoff Morgan (Geoff.Morgan@ncahs.health.nsw.gov.au) on a consultancy with NSW health to look at heatwaves, mortality and admissions. We use the percentiles of daily max temperature and apparent temperature in a similar way to the paper by Behnoosh Khalaj and Keith Dear. In additional sensitivity analyses we also developed material related to a newly proposed heatwave metric called the Excess Heat Factor by John Nairn at the BoM.

The reports/EHIs_transformations_doc.Rnw file is an Sweave document which contains the complete text and R codes that you can execute and produce the PDF (also found in the reports directory).  The interested reader is encouraged to run the R codes to do the calculations and generate the graphs that get compiled into that pdf file.  These R codes are also held separately in the src directory and can be evaluated in the correct sequence using the go.r script if you prefer.  Please don't hesitate to send me queries or comments on the algorithms or other aspects of this work.

Some Background
---------------

We were asked by our NSW health collaborators to investigate some heatwave indices developed by the BoM. NSW BoM like the look of three indices invented at the SA BoM office (by John Nairn) - they want to construct a national definition. Apparently BoM central HQ like John's definition the most (not published in a journal yet, the best ref is http://www.cawcr.gov.au/events/modelling_workshops/workshop_2009/papers/NAIRN.pdf). 

John has worked with PriceWaterhouseCoopers to apply the heatwave in a recent report http://www.pwc.com.au/industry/government/assets/extreme-heat-events-nov11.pdf

Ivan Hanigan
2012-04-21
#+end_src

* Codes
** ExcessHeatIndices-package.Rd
#+name:ExcessHeatIndices-package.Rd
#+begin_src markdown  :tangle man/ExcessHeatIndices-package.Rd :exports none :eval no
    \name{ExcessHeatIndices-package}
\alias{ExcessHeatIndices-package}
    \alias{ExcessHeatIndices}
\docType{package}
    \title{
Excess Heat Indices 
    ~~ package title ~~
}
    \description{
Excess Heat Indices for Human Health research
    ~~ A concise (1-5 lines) description of the package ~~
}
    \details{
\tabular{ll}{
    Package: \tab ExcessHeatIndices\cr
Type: \tab Package\cr
    Version: \tab 1.0\cr
Date: \tab 2013-01-30\cr
    License: \tab GPL2\cr
}
    ~~ An overview of how to use the package, including the most important functions ~~
}
    \author{
ivanhanigan
    
Maintainer: Who to complain to  ivan.hanigan@anu.edu.au 
    ~~ The author and/or maintainer of the package ~~
}
    \references{
~~ Literature or other references for background information ~~
    }

    \keyword{ package }
\seealso{
    ~~ Optional links to other man pages, e.g. ~~
~~ \code{\link[<pkg>:<pkg>-package]{<pkg>}} ~~
    }
\examples{
    ~~ simple examples of the most important functions ~~
}
    
#+end_src

** tests
#+name:tests
#+begin_src R :session *R* :tangle tests.r :exports none :eval no
  require(testthat)
  
  test_dir('tests', reporter = 'Summary')
  
#+end_src

** EHF
*** test
#+name:test-EHF
#+begin_src R :session *R* :tangle tests/test-EHF.r :exports none :eval no
  # first test
  dir()
  source('../R/EHF.r')
  require(swishdbtools)
  require(plyr)
  # access to ewedb is password restricted
  ch <- connect2postgres2('ewedb')
  slacode <- sql_subset(ch,"abs_sla.aussla01", subset = "sla_name = 'Scullin'",
             select = c("sla_code, sla_name"), eval=T)
  sql <- sql_subset(ch,"weather_sla.weather_sla",
                   subset=paste("sla_code = '",slacode$sla_code,"'",sep=""), eval = F)
  cat(sql)
  # this might take some minutes
  df <- dbGetQuery(ch, sql)
  head(df)
  tail(df)
  with(df, plot(date, maxave))
  str(df)
  df2 <- EHF(df, 'maxave', "date", min(df$date), max(df$date))
  names(df2)
  hist(subset(df2, EHF >= 1)[,'EHF'])
  threshold <- quantile(subset(df2, EHF >= 1)[,'EHF'], probs=0.9)
  
  with(df, plot(date, maxave, type = 'l'))
  with(subset(df2, EHF > threshold), points(date, maxave, col = 'red', pch = 16))
  
#+end_src

*** R
#+name:EHF
#+begin_src R :session *R* :tangle R/EHF.r :exports none :eval no
###############################################################################
 if (!require(Hmisc)) install.packages('Hmisc', repos='http://cran.csiro.au'); require(Hmisc)
 EHF <- function(analyte = data_subset,
  exposurename = 'air_temperature_in_degrees_c_max_climatezone_av',
  datename = 'date',
  referencePeriodStart = as.Date('1971-1-1'),
  referencePeriodEnd = as.Date('2000-12-31'),
  nlags = 32) {
  # TASK SHOULD WE IMPUTE MISSING DAYS?
 
  # first get lags
  # TASK THERE IS PROBABLY A VECTORISED VERSION THAT IS QUICKER?
  # TASK it is rollmean from the zoo package
  # ALTHOUGH THAT DOESNT HANDLE NAs SO TRY ROLLAPPLY?
  analyte$temp_lag0 <- analyte[,exposurename]
  exposuresList <- 'temp_lag0'
  # make sure in order
  analyte <- arrange(analyte,  analyte[,datename])
  # lag0 is not needed
  for(lagi in 1:nlags){
 	# lagi <- 1
 	exposuresList <- c(exposuresList, gsub('lag0',paste('lag', lagi,sep=''), exposuresList[1]))
 	analyte[,(ncol(analyte)+1)] <- Lag(analyte[,exposuresList[1]],lagi)
 	}
  exposuresList <- exposuresList[-1]
  names(analyte) <- c(names(analyte[,1:(ncol(analyte)-nlags)]),exposuresList)
  # head(analyte)
  # now 3 day av
  analyte$temp_movav <- rowMeans(analyte[,c('temp_lag0','temp_lag1','temp_lag2')], na.rm =FALSE)

  # now 30 day av
  # paste('temp_lag',3:32, sep = '', collapse = \"','\")
  analyte$temp30_movav <- rowMeans(analyte[,c('temp_lag3','temp_lag4','temp_lag5','temp_lag6','temp_lag7','temp_lag8','temp_lag9','temp_lag10','temp_lag11','temp_lag12','temp_lag13','temp_lag14','temp_lag15','temp_lag16','temp_lag17','temp_lag18','temp_lag19','temp_lag20','temp_lag21','temp_lag22','temp_lag23','temp_lag24','temp_lag25','temp_lag26','temp_lag27','temp_lag28','temp_lag29','temp_lag30','temp_lag31','temp_lag32')], na.rm =FALSE)
  # TASK note that this removes any missing days which could be imputed
  analyte <- na.omit(analyte)
  # head(analyte)
 
  # now calculate the EHI
  analyte$EHIaccl <- analyte$temp_movav - analyte$temp30_movav
  
  # first calculate the 95th centile
  referencestart <- referencePeriodStart
  referenceend <- referencePeriodEnd
  analyte$dateidCol <- analyte[,datename]
  reference <- subset(analyte, dateidCol >= referencestart & dateidCol <= referenceend, select = c('dateidCol', exposurename))
  head(reference);tail(reference)
  T95 <- quantile(reference[,exposurename], 0.95, na.rm = T)
  T95
 
  # now calculate the EHIsig
  analyte$EHIsig <- analyte$temp_movav - T95
  
  # now calculate the EHF
  analyte$EHF <- abs(analyte$EHIaccl) * analyte$EHIsig
  
  # proposed integrations
  # counts can be done quicker with this
  x <- analyte$EHIaccl >= 0
  xx <- (cumsum(!x) + 1) * x 
  x2<-(seq_along(x) - match(xx, xx) + 1) * x 
  analyte$EHIacclCount <- x2

  # alternately, slower but more interpretable
  # analyte$EHIacclCount2<-as.numeric(0)
  # # 
  # which(analyte$dates == as.Date('2009-1-1'))
  # which(analyte$dates == as.Date('2009-3-1'))
  
  # for(j in 43034:43093){
  # # j=43034
  # analyte$EHIacclCount2[j] <- ifelse(analyte$EHIaccl[j] < 0, 0,
  # ifelse(analyte$EHIaccl[j-1] >= 0, 1 + analyte$EHIacclCount2[j-1],
  # 1)
  # )
  # }
  
  x <- analyte$EHIsig >= 0
  xx <- (cumsum(!x) + 1) * x 
  x2<-(seq_along(x) - match(xx, xx) + 1) * x 
  analyte$EHIsigCount <- x2
  
  # sums
  EHFinverted  <- analyte$EHF * -1 
  y <- ifelse(EHFinverted >= 0, 0, analyte$EHF)
  f <- EHFinverted < 0
  f <- (cumsum(!f) + 1) * f 
  z <- unsplit(lapply(split(y,f),cumsum),f)
  analyte$EHFintegrated <- z
  
  # alternately, slower but more interpretable
  # analyte$EHFintegrated2 <- as.numeric(0)
  # for(j in 43034:43093){
  # # j = 43034
	# analyte$EHFintegrated2[j] <- ifelse(analyte$EHF[j] < 0,0,
	 # ifelse(analyte$EHF[j-1] >= 0,
	 # analyte$EHF[j] + analyte$EHFintegrated2[j-1],
	 # analyte$EHF[j])
	 # )
	# }
  
  return(analyte)
  }
 

#+end_src

*** man
#+name:EHF
#+begin_src markdown :tangle man/EHF.Rd :exports none :eval no
\name{EHF}
\alias{EHF}
%- Also NEED an '\alias' for EACH other topic documented here.
\title{
Excess Heat Factor
}
\description{
The EHF is an extension to a high pass filter, compared with long term percentiles.
}
\usage{
EHF(analyte = data_subset, exposurename = "air_temperature_in_degrees_c_max_climatezone_av", datename = "date", referencePeriodStart = as.Date("1971-1-1"), referencePeriodEnd = as.Date("2000-12-31"), nlags = 32)
}
%- maybe also 'usage' for other objects documented here.
\arguments{
  \item{analyte}{
dataframe
}
  \item{exposurename}{
the name of the exposure variable in the dataframe
}
  \item{datename}{
usually just date
}
  \item{referencePeriodStart}{
start of baseline climate reference period
}
  \item{referencePeriodEnd}{
end of baseline
}
  \item{nlags}{
number of lags, default is 32
}
}
\details{

}
\value{
A dataframe.
}
\references{
%% ~put references to the literature/web site here ~
}
\author{
ivanhanigan, original by John Nairn (Australian Bureau of Meteorology)
}
\note{
%%  ~~further notes~~
}



\seealso{
%% ~~objects to See Also as \code{\link{help}}, ~~~
}
\examples{

output <- EHF(analyte = data_subset, exposurename = "air_temperature_in_degrees_c_max_climatezone_av", 
    datename = "date", referencePeriodStart = as.Date("1971-1-1"), 
    referencePeriodEnd = as.Date("2000-12-31"), nlags = 32) 

}
% Add one or more standard keywords, see file 'KEYWORDS' in the
% R documentation directory.
\keyword{ ~kwd1 }
\keyword{ ~kwd2 }% __ONLY ONE__ keyword per line

#+end_src
* CaseStudies
** 2013-12-06-auto-download-bureau-meteorology-intra-day-data
#+name:auto-download-bureau-meteorology-intra-day-data-header
#+begin_src R :session *R* :tangle ~/projects/ivanhanigan.github.com.raw/_posts/2013-12-06-auto-download-bureau-meteorology-intra-day-data.md :exports none :eval no :padline no
  ---
  name: 2013-12-06-auto-download-bureau-meteorology-intra-day-data
  layout: post
  title: auto-download-bureau-meteorology-intra-day-data
  date: 2013-12-06
  categories:
  - extreme weather events
  - excess heat indices
  ---
  
  - We;re looking at health impacts of high temperatures at work 
  - need to see the highest temperatures during the working hours
  - bom provides hourly data for download, but only 3 days at a time
  - we build a script and set it on a schedule to run every day, download the data and collate the results
  
  #### First the FTP server URL structure
  
  - The URLS are predictable, just need the station id, state and a code if metro or rural
  
  #### table
      | Station_ID | State | City_9_or_regional_8_ |
      |      94774 | N     |                     9 |
      |      95719 | N     |                     8 |
      |      94768 | N     |                     9 |
      |      94763 | N     |                     9 |
      |      94767 | N     |                     9 |
      |      94910 | N     |                     8 |
      |      94929 | N     |                     8 |
      |      95896 | N     |                     8 |
      |      94693 | N     |                     8 |
      |      94691 | N     |                     8 |
      |      95677 | S     |                     9 |
      |      94675 | S     |                     9 |
      |      94672 | S     |                     9 |
      |      94866 | V     |                     9 |
      |      95867 | V     |                     9 |
      |      94868 | V     |                     9 |
      |      94875 | V     |                     8 |
  
  
  <p></p> 
  
  - now create a script called "bom_download.r"
  - it takes the station details and paste into the URLs
  - downloads the files
  - stores in a directory for each days downloads
  
  #### R Code:
      filename = "~/data/ExcessHeatIndices/inst/doc/weather_stations.csv"
      output_directory = "~/bom-downloads"
      setwd(output_directory)
      
      urls <- read.csv(filename)
      urls_list <- paste(sep = "", "http://www.bom.gov.au/fwo/ID",
                        urls$State,
                        "60", 
                        urls$City_9_or_regional_8_,
                        "01/ID",
                        urls$State,
                        "60",
                        urls$City_9_or_regional_8_,
                        "01.",
                        urls$Station_ID,
                        ".axf")
      
      output_directory <- file.path(output_directory,Sys.Date())
      dir.create(output_directory)
  
      for(url in urls_list)
      {
        output_file <- file.path(output_directory,basename(url))
        download.file(url, output_file, mode = "wb")
      
      }
      print("SUCCESS")
  
  <p></p> 
  
  - Now the data can be combined
  - clean up the header and extraneous extra line at the bottom
  
  #### R Code:
      # this takes data in directories from bom_download.r
       
      # first get list of directories
      filelist <- dir(pattern = "axf", recursive = T)
      filelist
       
      # next get directories for days we haven't done yet
      if(file.exists("complete_dataset.csv"))
      {
      complete_data <- read.csv("complete_dataset.csv", stringsAsFactors = F)
      #str(complete_data)
      last_collated <- max(as.Date(complete_data$date_downloaded))
      #max(complete_data$local_hrmin)
       
      days_downloaded <- dirname(filelist)
      filelist <- filelist[which(as.Date(days_downloaded) > as.Date(last_collated))]
      }
       
      # for these collate them into the complete file
      for(f in filelist)
      {
        f <- filelist[2]
        print(f)
        fin <- read.csv(f, colClasses = c("local_date_time_full.80." = "character"), stringsAsFactors = F, skip = 19)
        fin <- fin[1:(nrow(fin) - 1),]
        fin$date_downloaded <- dirname(f)
        fin$local_year <- substr(fin$local_date_time_full.80., 1, 4)
        fin$local_month <- substr(fin$local_date_time_full.80., 5, 6)
        fin$local_day <- substr(fin$local_date_time_full.80., 7, 8)
        fin$local_hrmin <- substr(fin$local_date_time_full.80., 9, 12)
        fin$local_date <- paste(fin$local_year, fin$local_month, fin$local_day, sep = "-")
        if(file.exists("complete_dataset.csv"))
        {
        write.table(fin, "complete_dataset.csv", row.names = F, sep = ",", append = T, col.names = F)
        } else {
        write.table(fin, "complete_dataset.csv", row.names = F, sep = ",")
        }
      }
  
  <p></p>
  
  - so now let;s automate the process
  - make a BAT file
  
  #### BAT file (windoze)
      "C:\Program Files\R\R-2.15.2\bin\Rscript.exe" "~\bom-downloads\bom_download.r"
  
  <p></p>
  
  - add this  bat file to the scheduled tasks in your control panel
  
  #### Conclusions
  
  - watch the data roll on in
  - each day there are about 3 days downloaded
  - meaning duplicates will be frequent, need to write a script to de-duplicate
  - cheers!
#+end_src
** table-stations-code
#+name:table-stations
#+begin_src R :session *R* :tangle no :exports reports :eval yes
  #### name:table-stations ####
  read.csv("inst/doc/weather_stations.csv")
#+end_src

